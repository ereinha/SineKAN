{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ereinha/SineKAN/blob/main/SineKAN_MNIST_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb5N8Ayea-QZ"
      },
      "outputs": [],
      "source": [
        "from sine_kan import *\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OYd8RAu5fSf"
      },
      "outputs": [],
      "source": [
        "# Load MNIST\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
        ")\n",
        "train_set = torchvision.datasets.MNIST(\n",
        "    root=\"./data\", train=True, download=True, transform=transform\n",
        ")\n",
        "val_set = torchvision.datasets.MNIST(\n",
        "    root=\"./data\", train=False, download=True, transform=transform\n",
        ")\n",
        "train_loader = DataLoader(train_set, batch_size=64, num_workers=2, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=64, num_workers=2, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7z9Q5gv73Ngn"
      },
      "outputs": [],
      "source": [
        "epochs = 30\n",
        "lrs = [1e-4, 2e-4, 3e-4, 4e-4, 5e-4]\n",
        "gammas = [0.7, 0.8, 0.9]\n",
        "hdims = [64, 128, 256]\n",
        "best_accs = []\n",
        "for lr in lrs:\n",
        "    for gamma in gammas:\n",
        "        for hdim in hdims:\n",
        "            best_acc = 0\n",
        "            # Define model\n",
        "            model = SineKAN(layers_hidden=[28 * 28, hdim, 10], grid_size=8)\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            model.to(device)\n",
        "            # Define optimizer\n",
        "            optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "            # Define learning rate scheduler\n",
        "            scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
        "\n",
        "            # Define loss\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            for epoch in range(epochs):\n",
        "                # Train\n",
        "                model.train()\n",
        "                with tqdm(train_loader) as pbar:\n",
        "                    for i, (images, labels) in enumerate(pbar):\n",
        "                        images = images.view(-1, 28 * 28).to(device)\n",
        "                        optimizer.zero_grad()\n",
        "                        output = model(images)\n",
        "                        loss = criterion(output, labels.to(device))\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        accuracy = (output.argmax(dim=1) == labels.to(device)).float().mean()\n",
        "                        pbar.set_postfix(loss=loss.item(), accuracy=accuracy.item(), lr=optimizer.param_groups[0]['lr'])\n",
        "\n",
        "                # Validation\n",
        "                model.eval()\n",
        "                val_loss = 0\n",
        "                val_accuracy = 0\n",
        "                with torch.no_grad():\n",
        "                    for images, labels in val_loader:\n",
        "                        images = images.view(-1, 28 * 28).to(device)\n",
        "                        output = model(images)\n",
        "                        val_loss += criterion(output, labels.to(device)).item()\n",
        "                        val_accuracy += (\n",
        "                            (output.argmax(dim=1) == labels.to(device)).float().mean().item()\n",
        "                        )\n",
        "                val_loss /= len(val_loader)\n",
        "                val_accuracy /= len(val_loader)\n",
        "                if val_accuracy > best_acc:\n",
        "                    best_acc = val_accuracy\n",
        "\n",
        "                # Update learning rate\n",
        "                scheduler.step()\n",
        "\n",
        "                print(\n",
        "                    f\"Epoch {epoch + 1}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}\"\n",
        "                )\n",
        "            best_accs.append(best_acc)\n",
        "            print(f\"LR: {lr} Gamma: {gamma} Hdim: {hdim} Best Accuracy: {best_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvJ4YGFGdyui"
      },
      "outputs": [],
      "source": [
        "hdims = [16, 32, 64, 128, 256, 512]\n",
        "epochs = 30\n",
        "val_accs = np.empty((6, epochs))\n",
        "for h, hdim in enumerate(hdims):\n",
        "    # Define model\n",
        "    model = SineKAN(layers_hidden=[28 * 28, hdim, 10], grid_size=8)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    # Define optimizer\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
        "    # Define learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "\n",
        "    # Define loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for epoch in range(epochs):\n",
        "        # Train\n",
        "        model.train()\n",
        "        with tqdm(train_loader) as pbar:\n",
        "            for i, (images, labels) in enumerate(pbar):\n",
        "                images = images.view(-1, 28 * 28).to(device)\n",
        "                optimizer.zero_grad()\n",
        "                output = model(images)\n",
        "                loss = criterion(output, labels.to(device))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                accuracy = (output.argmax(dim=1) == labels.to(device)).float().mean()\n",
        "                pbar.set_postfix(loss=loss.item(), accuracy=accuracy.item(), lr=optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_accuracy = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images = images.view(-1, 28 * 28).to(device)\n",
        "                output = model(images)\n",
        "                val_loss += criterion(output, labels.to(device)).item()\n",
        "                val_accuracy += (\n",
        "                    (output.argmax(dim=1) == labels.to(device)).float().mean().item()\n",
        "                )\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy /= len(val_loader)\n",
        "        val_accs[h, epoch] = val_accuracy\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch + 1}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hvytZJwUcri"
      },
      "outputs": [],
      "source": [
        "n_layers = [1, 2, 3, 4]\n",
        "epochs = 30\n",
        "layer_accs = np.empty((4, epochs))\n",
        "for h, n_layer in enumerate(n_layers):\n",
        "    # Define model\n",
        "    model = SineKAN(layers_hidden=[28 * 28] + [128]*n_layer + [10], grid_size=8)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    # Define optimizer\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
        "    # Define learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "\n",
        "    # Define loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for epoch in range(epochs):\n",
        "        # Train\n",
        "        model.train()\n",
        "        with tqdm(train_loader) as pbar:\n",
        "            for i, (images, labels) in enumerate(pbar):\n",
        "                images = images.view(-1, 28 * 28).to(device)\n",
        "                optimizer.zero_grad()\n",
        "                output = model(images)\n",
        "                loss = criterion(output, labels.to(device))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                accuracy = (output.argmax(dim=1) == labels.to(device)).float().mean()\n",
        "                pbar.set_postfix(loss=loss.item(), accuracy=accuracy.item(), lr=optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_accuracy = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images = images.view(-1, 28 * 28).to(device)\n",
        "                output = model(images)\n",
        "                val_loss += criterion(output, labels.to(device)).item()\n",
        "                val_accuracy += (\n",
        "                    (output.argmax(dim=1) == labels.to(device)).float().mean().item()\n",
        "                )\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy /= len(val_loader)\n",
        "        layer_accs[h, epoch] = val_accuracy\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch + 1}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lJho8zXO5em-"
      },
      "outputs": [],
      "source": [
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AlCgt3EHOtbB"
      },
      "outputs": [],
      "source": [
        "def profile_model(model, inputs, num_runs=1000):\n",
        "    total_times = []\n",
        "    for _ in range(num_runs):\n",
        "        with profile(activities=[ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
        "            model(inputs)\n",
        "        total_time = 0\n",
        "        # Extract total CUDA time spent on 'model_inference'\n",
        "        for event in prof.key_averages():\n",
        "            total_time += event.cuda_time_total\n",
        "        total_times.append(total_time)\n",
        "    mean = np.mean(total_times)\n",
        "    std = np.std(total_times)\n",
        "    print(f\"Mean CUDA time: {mean} micros\")\n",
        "    print(f\"Standard deviation of CUDA time: {std} micros\")\n",
        "    return mean, std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RpPdVaMb5h3B"
      },
      "outputs": [],
      "source": [
        "means = []\n",
        "stds = []\n",
        "inputs = torch.randn((1, 784)).to(device)\n",
        "hdims = [16, 32, 64, 128, 256, 512]\n",
        "for h in hdims:\n",
        "    model = SineKAN(layers_hidden=[28 * 28, h, 10], grid_size=8).to(device)\n",
        "    print('Hidden Dims: %d' % h)\n",
        "    mean, std = profile_model(model, inputs, 1000)\n",
        "    means.append(mean)\n",
        "    stds.append(std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ocQnoJHcSUYh"
      },
      "outputs": [],
      "source": [
        "means = []\n",
        "stds = []\n",
        "hdims = [16, 32, 64, 128, 256, 512]\n",
        "for h in hdims:\n",
        "    model = SineKAN(layers_hidden=[28 * 28, 128, 10], grid_size=8).to(device)\n",
        "    inputs = torch.randn((h, 784)).to(device)\n",
        "    print('Input Batch Size: %d' % h)\n",
        "    mean, std = profile_model(model, inputs, 1000)\n",
        "    means.append(mean)\n",
        "    stds.append(std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HZ9FGPPpg3ll"
      },
      "outputs": [],
      "source": [
        "means = []\n",
        "stds = []\n",
        "nlayers = [1, 2, 3, 4]\n",
        "for n in nlayers:\n",
        "    model = SineKAN(layers_hidden=[28 * 28] + [128]*n +[10], grid_size=8).to(device)\n",
        "    inputs = torch.randn((1, 784)).to(device)\n",
        "    print('Number of Hidden Layers: %d' % n)\n",
        "    mean, std = profile_model(model, inputs, 1000)\n",
        "    means.append(mean)\n",
        "    stds.append(std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xHM1JX7f5VdF"
      },
      "outputs": [],
      "source": [
        "means = []\n",
        "stds = []\n",
        "grid_sizes = [2, 4, 6, 8, 10, 12]\n",
        "for g in grid_sizes:\n",
        "    model = SineKAN(layers_hidden=[28 * 28, 128, 10], grid_size=g).to(device)\n",
        "    inputs = torch.randn((1, 784)).to(device)\n",
        "    print('Grid Size: %d' % n)\n",
        "    mean, std = profile_model(model, inputs, 1000)\n",
        "    means.append(mean)\n",
        "    stds.append(std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ExHCYJ0CYnAm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1s7qRW5h7NaCOadr36syQYidL_GimJ1w5",
      "authorship_tag": "ABX9TyOruDng9OOxc+r723xahz9k",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}